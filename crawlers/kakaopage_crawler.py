# crawlers/kakaopage_crawler.py
# ... (íŒŒì¼ ìƒë‹¨ì€ ì´ì „ê³¼ ë™ì¼) ...
import asyncio
import aiohttp
import json
import config
from tenacity import retry, stop_after_attempt, wait_exponential
from .base_crawler import ContentCrawler
from database import get_cursor

GRAPHQL_QUERY_ONGOING = """
query staticLandingDayOfWeekLayout($queryInput: StaticLandingDayOfWeekParamInput!) {
  staticLandingDayOfWeekLayout(input: $queryInput) {
    ...Layout
  }
}
fragment Layout on Layout {
  id, type, sections { ...Section }, screenUid
}
fragment Section on Section {
  id, uid, type, title
  ... on StaticLandingDayOfWeekSection {
    isEnd, totalCount
    items: groups {
      items {
        id, title, thumbnail, badgeList, statusBadge, ageGrade, seriesId
        authors { name, type }
      }
    }
  }
}
"""

GRAPHQL_QUERY_FINISHED = """
query staticLandingGenreSection($sectionId: ID!, $param: StaticLandingGenreParamInput!) {
  staticLandingGenreSection(sectionId: $sectionId, param: $param) {
    ... on StaticLandingGenreSection {
      isEnd, totalCount
      items: groups {
        items {
          id, title, thumbnail, badgeList, statusBadge, ageGrade, seriesId
          authors { name, type }
        }
      }
    }
  }
}
"""

DAY_TAB_UIDS = {
    'mon': '1', 'tue': '2', 'wed': '3', 'thu': '4', 'fri': '5', 'sat': '6', 'sun': '7',
    'hiatus': '8' # ğŸ‘ˆ 'íœ´ì¬' íƒ­ì—ì„œ ì°¾ì€ UID ê°’ìœ¼ë¡œ '8'ì„ ëŒ€ì²´í•˜ì„¸ìš”.
}

class KakaopageCrawler(ContentCrawler):
    def __init__(self):
        super().__init__('kakaopage')
        self.GRAPHQL_URL = 'https://page.kakao.com/graphql'
        self.HEADERS = {
            'User-Agent': config.CRAWLER_HEADERS['User-Agent'],
            'Content-Type': 'application/json',
            'Accept': 'application/graphql+json, application/json',
            'Referer': 'https://page.kakao.com/',
        }

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    async def _fetch_page_data(self, session, page, size=100, day_tab_uid=None, is_complete=False):
        if is_complete:
            query = GRAPHQL_QUERY_FINISHED
            variables = {"sectionId": "static-landing-Genre-section-Layout-10-0-view", "param": { "categoryUid": 10, "page": page, "size": size, "sortType": "view", "isComplete": True }}
        else:
            query = GRAPHQL_QUERY_ONGOING
            variables = {"queryInput": { "categoryUid": 10, "dayTabUid": day_tab_uid, "type": "Layout", "screenUid": 52, "page": page, "size": size }}

        payload = {"query": query, "variables": variables}
        try:
            async with session.post(self.GRAPHQL_URL, headers=self.HEADERS, json=payload, timeout=30) as response:
                response.raise_for_status()

                # ğŸ‘ˆ 2. aiohttp ë‚´ì¥ json íŒŒì„œ ì‚¬ìš©
                # content_type=None : 'application/graphql+json'ì„ jsonìœ¼ë¡œ ì¸ì‹ì‹œí‚´
                data = await response.json(content_type=None)

                data_root = data.get('data', {})
                if is_complete:
                    # ì™„ê²° íƒ­: data.staticLandingGenreSection.items[0].items
                    section = data_root.get('staticLandingGenreSection', {})
                    groups = section.get('items', [])
                else:
                    # ìš”ì¼ë³„ íƒ­: data.staticLandingDayOfWeekLayout.sections[0].items[0].items
                    layout = data_root.get('staticLandingDayOfWeekLayout', {})
                    sections = layout.get('sections', [])
                    groups = sections[0].get('items', []) if sections else [] # ğŸ‘ˆ 1. sections[0] ì•ˆì „í•˜ê²Œ ì ‘ê·¼

                # ğŸ‘ˆ 2. groups[0] ì•ˆì „í•˜ê²Œ ì ‘ê·¼
                items = groups[0].get('items', []) if groups else []
                return items
        except Exception as e:
            print(f"[{self.source_name}] Page {page} (day: {day_tab_uid}, complete: {is_complete}) ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    async def _fetch_ongoing_category(self, session, day_key, day_tab_uid, data_maps):
        print(f"[{self.source_name}] '{day_key}' (TabUID:{day_tab_uid}) ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘...")
        page = 1
        while True:
            try:
                items = await self._fetch_page_data(session, page=page, day_tab_uid=day_tab_uid)
                if not items: break
                for item in items:
                    content_id = str(item.get('seriesId'))
                    if content_id not in data_maps['all_content_today']:
                        data_maps['all_content_today'][content_id] = item
                        item['normalized_weekdays'] = set()
                    data_maps['all_content_today'][content_id]['normalized_weekdays'].add(day_key)
                    if 'íœ´ì¬' in (item.get('statusBadge') or ''): data_maps['hiatus_today'][content_id] = item
                    else: data_maps['ongoing_today'][content_id] = item
                page += 1
                await asyncio.sleep(0.1)
            except Exception:
                print(f"[{self.source_name}] '{day_key}' í˜ì´ì§€ {page}ì—ì„œ ìµœì¢… ì‹¤íŒ¨.")
                break
        print(f"[{self.source_name}] '{day_key}' ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.")

    async def fetch_all_data(self):
        print(f"[{self.source_name}] ì„œë²„ì—ì„œ ìµœì‹  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤...")
        data_maps = {'all_content_today': {}, 'ongoing_today': {}, 'hiatus_today': {}, 'finished_today': {}}
        async with aiohttp.ClientSession() as session:
            await asyncio.gather(*[self._fetch_ongoing_category(session, day, uid, data_maps) for day, uid in DAY_TAB_UIDS.items()])
            print(f"[{self.source_name}] 'ì™„ê²°' ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘...")
            total_finished_found = 0 # ğŸ‘ˆ 1. ìˆ˜ì§‘í•œ ì™„ê²°ì‘ ì´ ê°œìˆ˜
            for page in range(1, 250): # ìµœëŒ€ 25000ê°œ (100 * 250)
                try:
                    items = await self._fetch_page_data(session, page=page, size=100, is_complete=True)
                    if not items:
                        print(f"[{self.source_name}] 'ì™„ê²°' ëª©ë¡ {page-1} í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘ ì¢…ë£Œ.")
                        break # ğŸ‘ˆ 2. ë°ì´í„° ì—†ìœ¼ë©´ ë£¨í”„ íƒˆì¶œ

                    for item in items:
                        content_id = str(item.get('seriesId'))
                        total_finished_found += 1 # ğŸ‘ˆ 3. DB ì €ì¥ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ë¬´ì¡°ê±´ ì¹´ìš´íŠ¸

                        # (1) ì „ì²´ ëª©ë¡ì— ì¶”ê°€ (ì‹ ê·œì¸ ê²½ìš°)
                        if content_id not in data_maps['all_content_today']:
                            data_maps['all_content_today'][content_id] = item

                        # (2) 'finished_today' ë§µì— ë¬´ì¡°ê±´ ì¶”ê°€ (ê°€ì¥ ì¤‘ìš”)
                        # ğŸ‘ˆ 4. 'ì—°ì¬ì¤‘'ì´ì—ˆë‹¤ê°€ ì™„ê²°ëœ ì‘í’ˆë„ ì—¬ê¸°ì— í¬í•¨ë˜ì–´ì•¼ í•¨
                        data_maps['finished_today'][content_id] = item

                    print(f"[{self.source_name}] 'ì™„ê²°' {page} í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ (ëˆ„ì  {total_finished_found}ê°œ)")

                    if total_finished_found >= 2000: # ğŸ‘ˆ 5. ì´ ìˆ˜ì§‘ ê°œìˆ˜ë¡œ 2000ê°œ ëŒíŒŒ í™•ì¸
                        print(f"[{self.source_name}] ëª©í‘œ ìˆ˜ëŸ‰({total_finished_found}ê°œ) ë‹¬ì„±. ìˆ˜ì§‘ ì¢…ë£Œ.")
                        break
                except Exception:
                    print(f"[{self.source_name}] 'ì™„ê²°' í˜ì´ì§€ {page}ì—ì„œ ìµœì¢… ì‹¤íŒ¨.")
                    break
            for content in data_maps['all_content_today'].values():
                if 'normalized_weekdays' in content: content['normalized_weekdays'] = list(content['normalized_weekdays'])
        print(f"[{self.source_name}] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(data_maps['all_content_today'])}ê°œ ê³ ìœ  ì½˜í…ì¸  í™•ì¸")
        return (data_maps['ongoing_today'], data_maps['hiatus_today'], data_maps['finished_today'], data_maps['all_content_today'])

    def synchronize_database(self, conn, all_content_today, ongoing_today, hiatus_today, finished_today):
        print(f"\n[{self.source_name}] DB ë™ê¸°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        cursor = get_cursor(conn)
        # [BUG FIX] ì—…ë°ì´íŠ¸ ë¡œì§ ê°•í™”ë¥¼ ìœ„í•´ statusì™€ metaë„ í•¨ê»˜ ì¡°íšŒ
        cursor.execute("SELECT content_id, title, status, meta FROM contents WHERE source = %s", (self.source_name,))
        db_data = {row['content_id']: {'title': row['title'], 'status': row['status'], 'meta': row['meta']} for row in cursor.fetchall()}

        updates, inserts, unique_inserts_count = [], [], 0

        for cid, cdata in all_content_today.items():
            status = 'ì™„ê²°' if cid in finished_today else 'íœ´ì¬' if cid in hiatus_today else 'ì—°ì¬ì¤‘'
            title = cdata.get('title', 'ì œëª© ì—†ìŒ')

            # [ìˆ˜ì •] naver_webtoon_crawler.pyì™€ ë™ì¼í•œ í‘œì¤€ meta êµ¬ì¡°ë¡œ ë³€ê²½
            meta = {
                "common": {
                    "authors": [a.get('name') for a in cdata.get('authors', []) if a.get('name')],
                    "thumbnail_url": cdata.get('thumbnail')
                },
                "attributes": {
                    "weekdays": cdata.get('normalized_weekdays', [])
                }
            }

            if cid in db_data:
                # [BUG FIX] ì œëª©, ìƒíƒœ, ë©”íƒ€ë°ì´í„° ì¤‘ í•˜ë‚˜ë¼ë„ ë³€ê²½ë˜ë©´ ì—…ë°ì´íŠ¸
                db_item = db_data[cid]
                if db_item['title'] != title or db_item['status'] != status or db_item['meta'] != meta:
                    updates.append(('webtoon', title, status, json.dumps(meta), cid, self.source_name))
            else:
                inserts.append((cid, self.source_name, 'webtoon', title, status, json.dumps(meta)))

        if updates:
            cursor.executemany("UPDATE contents SET content_type=%s, title=%s, status=%s, meta=%s WHERE content_id=%s AND source=%s", updates)
            print(f"[{self.source_name}] {len(updates)}ê°œ ì½˜í…ì¸  ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        if inserts:
            seen, u_inserts = set(), []
            for i in inserts:
                k = (i[0], i[1])
                if k not in seen: u_inserts.append(i); seen.add(k)
            cursor.executemany("INSERT INTO contents (content_id, source, content_type, title, status, meta) VALUES (%s, %s, %s, %s, %s, %s)", u_inserts)
            unique_inserts_count = len(u_inserts)
            print(f"[{self.source_name}] {unique_inserts_count}ê°œ ì‹ ê·œ ì½˜í…ì¸  DB ì¶”ê°€ ì™„ë£Œ.")
        conn.commit(); cursor.close(); print(f"[{self.source_name}] DB ë™ê¸°í™” ì™„ë£Œ.")
        return unique_inserts_count

    async def run_daily_check(self, conn):
        from services.notification_service import send_completion_notifications
        print(f"LOG: [{self.source_name}] ì¼ì¼ ì ê²€ ì‹œì‘...")
        cursor = get_cursor(conn)
        cursor.execute("SELECT content_id, status FROM contents WHERE source = %s", (self.source_name,))
        db_state = {row['content_id']: row['status'] for row in cursor.fetchall()}
        cursor.close()
        ongoing, hiatus, finished, all_content = await self.fetch_all_data()
        newly_completed = {cid for cid, s in db_state.items() if s != 'ì™„ê²°' and cid in finished}
        print(f"LOG: [{self.source_name}] {len(newly_completed)}ê°œ ì‹ ê·œ ì™„ê²° ì½˜í…ì¸  ë°œê²¬.")
        details, notified = [], 0
        if newly_completed:
            for cid in newly_completed:
                if cid in all_content and 'title' in all_content[cid]:
                    all_content[cid]['titleName'] = all_content[cid]['title']
            try:
                details, notified = send_completion_notifications(get_cursor(conn), newly_completed, all_content, self.source_name)
            except ValueError as e:
                print(f"ê²½ê³ : [{self.source_name}] ì•Œë¦¼ ë°œì†¡ ë¶ˆê°€: {e}")
        added = self.synchronize_database(conn, all_content, ongoing, hiatus, finished)
        print(f"LOG: [{self.source_name}] ì¼ì¼ ì ê²€ ì™„ë£Œ.")
        return added, details, notified

if __name__ == '__main__':
    print("KakaopageCrawler êµ¬í˜„ íŒŒì¼ì…ë‹ˆë‹¤. ì§ì ‘ ì‹¤í–‰ ì‹œ ë³„ë„ ë™ì‘ì€ ì—†ìŠµë‹ˆë‹¤.")
